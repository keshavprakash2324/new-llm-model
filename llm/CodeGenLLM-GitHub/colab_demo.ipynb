## ðŸ§ª CodeGenLLM Colab Demo

```python
# Install dependencies
!pip install torch gradio

# Define model and tokenizer (use existing code)
from tokenizer.tokenizer import ByteTokenizer
from model.model import CodeGenLLM
from inference.infer import generate

# Load model
vocab_size = 256
model = CodeGenLLM(vocab_size)
model.load_state_dict(torch.load("model_weights.pth", map_location="cpu"))
model.eval()
tokenizer = ByteTokenizer()

# Generate code
prompt = "def fibonacci(n):"
print(generate(model, tokenizer, prompt))
```